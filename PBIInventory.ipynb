{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","import sys  # For simplicity, we'll read config file from 1st CLI param sys.argv[1]\n","import json\n","import logging\n","import requests\n","import msal\n","import pandas as pd\n","from datetime import date, timedelta,datetime\n","from delta.tables import *\n","from pyspark.sql.functions import count_distinct,col,year,month\n","from pyspark.sql import functions as F\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n","spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"13edf138-68be-4675-a60b-fc9d09db9243","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-08T15:23:45.1955981Z","session_start_time":null,"execution_start_time":"2023-08-08T15:23:45.5523434Z","execution_finish_time":"2023-08-08T15:23:45.8441338Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"8bc92eae-fdf3-453d-94d5-57d801ee7b27"},"text/plain":"StatementMeta(, 13edf138-68be-4675-a60b-fc9d09db9243, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{},"id":"5e30fc83-e645-47d5-8251-c836558a9d6d"},{"cell_type":"code","source":["def createDeltaTables(tableName,sdf):\n","    from pyspark.sql.functions import col,year,month\n","    from pyspark.sql import functions as F\n","    from datetime import datetime\n","    sdf = sdf.withColumn('Year', F.lit(datetime.now().strftime(\"%Y\")))\n","    sdf = sdf.withColumn('Month', F.lit(datetime.now().strftime(\"%m\")))\n","    sdf = sdf.withColumn('RunDate', F.lit(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")))\n","    sdf.write.mode(\"append\").option(\"mergeSchema\",\"true\").format(\"parquet\").save(\"Files/Stage/\"+ datetime.now().strftime(\"%Y/%m/%d\") + \"/\" + tableName)\n","    return 1"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e8b5a59-5963-4148-a31b-333453510abf","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T14:37:31.4854654Z","session_start_time":null,"execution_start_time":"2023-08-07T14:37:31.8957685Z","execution_finish_time":"2023-08-07T14:37:32.1857786Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"bc8e598f-71e8-4624-9efe-489e05d4581e"},"text/plain":"StatementMeta(, 1e8b5a59-5963-4148-a31b-333453510abf, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0d2ace54-b961-4ebe-9db9-1f6ce7cf194d"},{"cell_type":"code","source":["def createDeltaTablesV1(tableName,key):\n","    from pyspark.sql.functions import col,year,month\n","    from pyspark.sql import functions as F\n","\n","    vDate=date.today().strftime(\"%Y/%m/%d\")\n","\n","    if DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\"+tableName):\n","        toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + tableName + \"/\"\n","        sourcePath=\"Files/Stage/\" + vDate + \"/\" + tableName\n","        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n","        sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n","        sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n","        \n","        if tableName==\"Lakehouses\" or tableName==\"Warehouses\" or tableName==\"Synapsenotebooks\":\n","            sdfSource = sdfSource.filter(sdfSource.id!=\"NaN\")            \n","\n","        sdfSource=sdfSource.drop(\"RunDate\")\n","        sdfSource=sdfSource.distinct()\n","        sdfSource=sdfSource.withColumn('RunDate', F.lit(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"))) \n","\n","        (sdfToUpdate.alias(\"t\")\n","        .merge(sdfSource.alias(\"s\"), key)\n","        .whenMatchedUpdateAll()\n","        .whenNotMatchedInsertAll()\n","        .execute()\n","        )\n","    else: \n","           # sourcePath=\"Files/Stage/2023/07/21/\" + tableName \n","            sourcePath=\"Files/Stage/\" + datetime.now().strftime(\"%Y/%m/%d\") + \"/\" + tableName \n","            sdf = spark.read.format(\"parquet\").load(sourcePath)\n","            sdf.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").format(\"delta\").save(\"Tables/\" + tableName)\n","    return 1"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"13edf138-68be-4675-a60b-fc9d09db9243","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-08T15:23:38.5411641Z","session_start_time":null,"execution_start_time":"2023-08-08T15:23:38.9244604Z","execution_finish_time":"2023-08-08T15:23:39.247034Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"833aac4e-cc6d-4f76-b4c9-ff28fade2b3e"},"text/plain":"StatementMeta(, 13edf138-68be-4675-a60b-fc9d09db9243, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e41ed68a-73af-4b3a-8e91-1c61f1a3950b"},{"cell_type":"code","source":["from datetime import datetime, timedelta\n","url=None\n","\n","if DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n","    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n","    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n","    # Begin Date -5 date to go back 5 days. \n","    Begindate = Begindate + timedelta(days=-2)\n","    print(Begindate)\n","    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n","    print(Begindate)\n","    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n","    print(url) "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e8b5a59-5963-4148-a31b-333453510abf","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T14:37:47.9856565Z","session_start_time":null,"execution_start_time":"2023-08-07T14:37:48.4081437Z","execution_finish_time":"2023-08-07T14:38:04.9000165Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":6,"FAILED":0},"jobs":[{"displayName":"first at /tmp/ipykernel_7975/1414904461.py:5","dataWritten":0,"dataRead":393,"rowCount":5,"usageDescription":"","jobId":13,"name":"first at /tmp/ipykernel_7975/1414904461.py:5","description":"Job group for statement 6:\nfrom datetime import datetime, timedelta\nurl=None\n\nif DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n    # Begin Date -5 date to go back 5 days. \n    Begindate = Begindate + timedelta(days=-2)\n    print(Begindate)\n    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n    print(Begindate)\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n    print(url) ","submissionTime":"2023-08-07T14:38:04.287GMT","completionTime":"2023-08-07T14:38:04.317GMT","stageIds":[21,22],"jobGroup":"6","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"first at /tmp/ipykernel_7975/1414904461.py:5","dataWritten":393,"dataRead":214878,"rowCount":178302,"usageDescription":"","jobId":12,"name":"first at /tmp/ipykernel_7975/1414904461.py:5","description":"Job group for statement 6:\nfrom datetime import datetime, timedelta\nurl=None\n\nif DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n    # Begin Date -5 date to go back 5 days. \n    Begindate = Begindate + timedelta(days=-2)\n    print(Begindate)\n    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n    print(Begindate)\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n    print(url) ","submissionTime":"2023-08-07T14:38:03.899GMT","completionTime":"2023-08-07T14:38:04.277GMT","stageIds":[20],"jobGroup":"6","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"first at /tmp/ipykernel_7975/1414904461.py:5","dataWritten":0,"dataRead":3924,"rowCount":6,"usageDescription":"","jobId":11,"name":"first at /tmp/ipykernel_7975/1414904461.py:5","description":"Delta: Job group for statement 6:\nfrom datetime import datetime, timedelta\nurl=None\n\nif DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n    # Begin Date -5 date to go back 5 days. \n    Begindate = Begindate + timedelta(days=-2)\n    print(Begindate)\n    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n    print(Begindate)\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n    print(url) : Filtering files for query","submissionTime":"2023-08-07T14:38:03.462GMT","completionTime":"2023-08-07T14:38:03.770GMT","stageIds":[19,18],"jobGroup":"6","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4463,"rowCount":50,"usageDescription":"","jobId":10,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nfrom datetime import datetime, timedelta\nurl=None\n\nif DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n    # Begin Date -5 date to go back 5 days. \n    Begindate = Begindate + timedelta(days=-2)\n    print(Begindate)\n    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n    print(Begindate)\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n    print(url) : Compute snapshot for version: 3","submissionTime":"2023-08-07T14:37:51.147GMT","completionTime":"2023-08-07T14:37:51.217GMT","stageIds":[15,16,17],"jobGroup":"6","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":54,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4463,"dataRead":15248,"rowCount":74,"usageDescription":"","jobId":9,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nfrom datetime import datetime, timedelta\nurl=None\n\nif DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n    # Begin Date -5 date to go back 5 days. \n    Begindate = Begindate + timedelta(days=-2)\n    print(Begindate)\n    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n    print(Begindate)\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n    print(url) : Compute snapshot for version: 3","submissionTime":"2023-08-07T14:37:50.103GMT","completionTime":"2023-08-07T14:37:51.113GMT","stageIds":[13,14],"jobGroup":"6","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":15248,"dataRead":17505,"rowCount":48,"usageDescription":"","jobId":8,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nfrom datetime import datetime, timedelta\nurl=None\n\nif DeltaTable.isDeltaTable(spark, \"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/Workspaces\"):\n    RunDate=spark.sql(\"select max(RunDate) from Workspaces\").first()[\"max(RunDate)\"]\n    Begindate = datetime.strptime(RunDate, \"%Y-%m-%dT%H:%M:%S\") \n    # Begin Date -5 date to go back 5 days. \n    Begindate = Begindate + timedelta(days=-2)\n    print(Begindate)\n    Begindate=Begindate.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n    print(Begindate)\n    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=\" + Begindate +\"0Z\"\n    print(url) : Compute snapshot for version: 3","submissionTime":"2023-08-07T14:37:49.249GMT","completionTime":"2023-08-07T14:37:49.749GMT","stageIds":[12],"jobGroup":"6","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"1b96b483-97a4-47e5-b473-ee76725f9548"},"text/plain":"StatementMeta(, 1e8b5a59-5963-4148-a31b-333453510abf, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2023-07-26 15:37:21\n2023-07-26T15:37:21.000000\nhttps://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=2023-07-26T15:37:21.0000000Z\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"99070009-53e9-4801-913e-af92d20569d1"},{"cell_type":"code","source":["def getToken():\n","    import requests\n","    import msal\n","    config = {\n","                \"authority\": \"https://login.microsoftonline.com/ReplaceWithYourTenant\",\n","                \"client_id\": \"ReplaceWithYourClientId\",\n","                \"scope\": [\"https://analysis.windows.net/powerbi/api/.default\"],\n","                \"secret\": \"ReplaceWithYourClient'sSceret\",\n","                \"endpoint\": \"https://api.powerbi.com/v1.0/myorg/admin/activityevents\"\n","            }\n","\n","#Get yesterdays date and convert to string\n","\n","\n","# Create a preferably long-lived app instance which maintains a token cache.\n","    app = msal.ConfidentialClientApplication(\n","            config[\"client_id\"], authority=config[\"authority\"],\n","            client_credential=config[\"secret\"],\n","            )\n","\n","        # The pattern to acquire a token looks like this.\n","    result = None\n","\n","        # Firstly, looks up a token from cache\n","        # Since we are looking for token for the current app, NOT for an end user,\n","        # notice we give account parameter as None.\n","    result = app.acquire_token_silent(config[\"scope\"], account=None)\n","\n","    if not result:\n","        logging.info(\"No suitable token exists in cache. Let's get a new one from AAD.\")\n","        result = app.acquire_token_for_client(scopes=config[\"scope\"])\n","\n","    if \"access_token\" in result:\n","            # Calling graph using the access token\n","        logging.info(\"Got the Token\")\n","\n","    else:\n","        print(result.get(\"error\"))\n","        print(result.get(\"error_description\"))\n","        print(result.get(\"correlation_id\")) \n","    \n","    return result\n","\n","result=getToken()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e8b5a59-5963-4148-a31b-333453510abf","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T14:37:59.6836741Z","session_start_time":null,"execution_start_time":"2023-08-07T14:38:05.456615Z","execution_finish_time":"2023-08-07T14:38:05.7539449Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"26ffc799-7845-45bd-822c-1dd5e88a6b0e"},"text/plain":"StatementMeta(, 1e8b5a59-5963-4148-a31b-333453510abf, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ade8b11a-2286-4771-94a6-a84faa55e0d1"},{"cell_type":"code","source":["#print(result)\n","activityDate = date.today() \n","\n","#url=None\n","#Set Power BI REST API to get Activities for today\n","if not url:\n","    #url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces=True\" \n","    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified\" \n","\n","print(url)\n","\n","access_token = result['access_token']\n","header = {'Content-Type':'application/json', 'Authorization':f'Bearer {access_token}'}\n","response = requests.get(url=url, headers=header)\n","\n","if response.status_code ==200 :\n","    response_obj = response.json()\n","    #print(response_obj)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e8b5a59-5963-4148-a31b-333453510abf","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T14:38:13.2063049Z","session_start_time":null,"execution_start_time":"2023-08-07T14:38:13.6367195Z","execution_finish_time":"2023-08-07T14:38:14.5463997Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"413db01e-1dc6-47ae-997d-71d034265178"},"text/plain":"StatementMeta(, 1e8b5a59-5963-4148-a31b-333453510abf, 8, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?modifiedSince=2023-07-26T15:37:21.0000000Z\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ba3b340b-cb6d-459f-95db-6f0b7b7aa254"},{"cell_type":"code","source":["#response.status_code\n","#print(response.text)\n","print(f\"Total Workspaces to iterate {len(response_obj)}\")\n","#print(response_obj)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e8b5a59-5963-4148-a31b-333453510abf","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T14:38:22.5920313Z","session_start_time":null,"execution_start_time":"2023-08-07T14:38:22.9721264Z","execution_finish_time":"2023-08-07T14:38:23.2922404Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"655db9ee-e22a-429e-9983-8e1d78c50caf"},"text/plain":"StatementMeta(, 1e8b5a59-5963-4148-a31b-333453510abf, 9, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Total Workspaces to iterate 1530\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5bc78aa8-b828-4699-8bfb-a39953ee4d5f"},{"cell_type":"code","source":["# Printing total workspaces to iterate \n","from pyspark.sql.types import *\n","\n","print(f\"Total Workspaces to iterate {len(response_obj)}\")\n","\n","from time import sleep \n","for i in range(0,len(response_obj),100): #for i in range(0,100,100):\n","    print(f\"Iterating workpace Number {(i+100)}\")\n","    workspaces=response_obj[i:i+100]\n","    workspaceList = [] \n","    for item in workspaces:\n","        workspaceList.append(item['id'])\n","        WorkspaceListToSend= {\"workspaces\":workspaceList}\n","        #print(WorkspaceListToSend)\n","    ################# Sending the pst request to get workspaces details #####################################################################    \n","    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo?lineage=true&datasourceDetails=true&getArtifactUsers=true\" \n","    \n","    result=getToken()\n","    access_token = result['access_token']\n","    header = {'Content-Type':'application/json', 'Authorization':f'Bearer {access_token}'}\n","\n","    scan = requests.post(url,headers=header,json=WorkspaceListToSend)\n","    print(scan.status_code)\n","    sleep(0.2)\n","    scanResponse=json.loads(scan.text)\n","    print(scanResponse)\n","\n","    waitForScanToComplete =True\n","    #print(scan.id) \n","    scanId = scanResponse['id']\n","    #####################Checking to see if the Scan completed, at this time not coding for any other status than Succeded, but may have to look at that as well. \n","    while waitForScanToComplete:\n","        url=\"https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/\" + scanId\n","        status = requests.get(url,headers=header) \n","        statusResponse=json.loads(status.text)\n","        print(statusResponse)\n","        statusState = statusResponse['status']\n","        print(statusState)\n","        if statusState==\"Succeeded\":\n","            break \n","        sleep(0.2)\n","    # The status has changed now to succeeded, get the result\n","    url = \"https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/\" + scanId\n","    scanResult = requests.get(url,headers=header)\n","\n","    ################################# Converting Text response to json object and creating pandas dataframe ############################################\n","    print(scanResult)\n","    scanResultResponse=scanResult.text\n","    #print(scanResultResponse)\n","    print(type(scanResultResponse))\n","    data = json.loads(scanResultResponse)\n","    df=pd.DataFrame(data['workspaces'])\n","\n","    lakehouse = pd.DataFrame()\n","    warehouses = pd.DataFrame()\n","    synapsenotebook = pd.DataFrame()\n","    reports = pd.DataFrame()\n","    reportUsers=pd.DataFrame()\n","    dashboards = pd.DataFrame()\n","    dashboardUsers=pd.DataFrame()\n","    datasets = pd.DataFrame()\n","    datasetUsers=pd.DataFrame()\n","    dataflows = pd.DataFrame()\n","    dataflowUsers=pd.DataFrame()\n","    workspaceUsers = pd.DataFrame()\n","    dfdatasourceInstances = pd.DataFrame()\n","    \n","    if 'users' in df.columns:\n","       try:\n","            workspaceUsers = pd.json_normalize(data=data['workspaces'],record_path=['users'],record_prefix='users_',meta=\"id\")\n","       except:\n","            pass\n","    \n","    try:\n","        if 'datasourceInstances' in data:\n","            dfdatasourceInstances=pd.DataFrame(data['datasourceInstances'])\n","            datasourceInstances_schema=StructType([StructField(\"datasourceType\",StringType(),True),\n","                StructField(\"connectionDetails\",StringType(),True),\n","                StructField(\"datasourceId\",StringType(),True),\n","                StructField(\"gatewayId\",StringType(),True)\n","                ]) \n","    except:\n","            pass\n","    ##################################################################################################################################\n","\n","    if 'reports' in df.columns:\n","        try:\n","            reports = pd.json_normalize(data=data['workspaces'],record_path=['reports'],record_prefix='reports_',meta=\"id\")\n","            if 'reports_users' in reports.columns:\n","                reportUsers=reports[['reports_id','reports_users']]\n","                reportUsers=pd.DataFrame(reportUsers).explode(\"reports_users\")\n","                reportUsers=pd.concat([reportUsers,reportUsers.pop(\"reports_users\").apply(pd.Series)],axis=1)\n","        except:\n","            pass\n","\n","    if 'dashboards' in df.columns:\n","        try:\n","            dashboards = pd.json_normalize(data=data['workspaces'],record_path=['dashboards'],record_prefix='dashboards_',meta=\"id\")\n","            if 'dashboards_users' in dashboards.columns:\n","                dashboardUsers=dashboards[['dashboards_id','dashboards_users']]\n","                dashboardUsers=pd.DataFrame(dashboardUsers).explode(\"dashboards_users\")\n","                dashboardUsers=pd.concat([dashboardUsers,dashboardUsers.pop(\"dashboards_users\").apply(pd.Series)],axis=1)\n","        except:\n","            pass\n","    \n","    if 'datasets' in df.columns:\n","        try:\n","            datasets = pd.json_normalize(data=data['workspaces'],record_path=['datasets'],record_prefix='datasets_',meta=\"id\")\n","            if 'datasets_users' in datasets.columns:\n","                datasetUsers=datasets[['datasets_id','datasets_users']]\n","                datasetUsers=pd.DataFrame(datasetUsers).explode(\"datasets_users\")\n","                datasetUsers=pd.concat([datasetUsers,datasetUsers.pop(\"datasets_users\").apply(pd.Series)],axis=1)\n","        except:\n","            pass\n","    \n","    if 'dataflows' in df.columns:\n","        try:\n","            dataflows = pd.json_normalize(data=data['workspaces'],record_path=['dataflows'],record_prefix='dataflows_',meta=\"id\")\n","            if 'dataflows_users' in dataflows.columns:\n","                dataflowUsers=dataflows[['dataflows_objectId','dataflows_users']]\n","                dataflowUsers=pd.DataFrame(dataflowUsers).explode(\"dataflows_users\")\n","                dataflowUsers=pd.concat([dataflowUsers,dataflowUsers.pop(\"dataflows_users\").apply(pd.Series)],axis=1)\n","        except:\n","            pass\n","\n","    \n","    ################################# Define the structure for the Spark Tables ############################################\n","   \n","   \n","    if not reports.empty:\n","        reports_schema = StructType()\n","        for column in reports:\n","            reports_schema.add(str(column),StringType())\n","\n","    if not reportUsers.empty:\n","        reportUsers_schema = StructType()\n","        for column in reportUsers:\n","            reportUsers_schema.add(str(column),StringType())\n","    \n","\n","    if not dashboards.empty:\n","        dashboards_schema = StructType()\n","        for column in dashboards:\n","            dashboards_schema.add(str(column),StringType())\n","     \n","    if not dashboardUsers.empty:\n","        dashboardUsers_schema = StructType()\n","        for column in dashboardUsers:\n","            dashboardUsers_schema.add(str(column),StringType())\n","     \n","    if not datasets.empty:\n","        datasets_schema = StructType()\n","        for column in datasets:\n","            datasets_schema.add(str(column),StringType())\n","    \n","    if not datasetUsers.empty:\n","        datasetUsers_schema = StructType()\n","        for column in datasetUsers:\n","            datasetUsers_schema.add(str(column),StringType())\n","    \n","    if not dataflows.empty:\n","        dataflow_schema= StructType()\n","        for column in dataflows:\n","            dataflow_schema.add(str(column),StringType())\n","    \n","    if not dataflowUsers.empty:\n","        dataflowUsers_schema= StructType()\n","        for column in dataflowUsers:\n","            dataflowUsers_schema.add(str(column),StringType())\n","    \n","    if not workspaceUsers.empty:\n","        workspaceUsers_schema= StructType()\n","        for column in workspaceUsers:\n","            workspaceUsers_schema.add(str(column),StringType())\n","\n","    # May be we should drop these columns ,'lakehouse','warehouses','synapsenotebook'\n","    workspaces=df.drop(['reports','dashboards','datasets','dataflows','datamarts','users'],axis=1,errors='ignore')\n","\n","    try:\n","        if 'lakehouse' in df.columns:\n","            lakehouse=workspaces[['id','lakehouse']]\n","            lakehouse.rename(columns={\"id\":\"workspaceid\"},inplace=True)\n","            lakehouse=pd.DataFrame(lakehouse).explode(\"lakehouse\")\n","            lakehouse=pd.concat([lakehouse,lakehouse.pop(\"lakehouse\").apply(pd.Series)],axis=1).drop(columns=0)\n","    except:\n","            pass\n","\n","    try:\n","        if 'warehouses' in df.columns:\n","            warehouses=workspaces[['id','warehouses']]\n","            warehouses.rename(columns={\"id\":\"workspaceid\"},inplace=True)\n","            warehouses=pd.DataFrame(warehouses).explode(\"warehouses\")\n","            warehouses=pd.concat([warehouses,warehouses.pop(\"warehouses\").apply(pd.Series)],axis=1).drop(columns=0)\n","    except:\n","            pass\n","\n","    try:\n","        if 'synapsenotebook' in df.columns:\n","            synapsenotebook=workspaces[['id','synapsenotebook']]\n","            synapsenotebook.rename(columns={\"id\":\"workspaceid\"},inplace=True)\n","            synapsenotebook=pd.DataFrame(synapsenotebook).explode(\"synapsenotebook\")\n","            synapsenotebook=pd.concat([synapsenotebook,synapsenotebook.pop(\"synapsenotebook\").apply(pd.Series)],axis=1).drop(columns=0)\n","    except:\n","            pass\n","\n","    if not workspaces.empty:\n","            workspace_schema= StructType()\n","            for column in workspaces:\n","                workspace_schema.add(str(column),StringType())\n","\n","    if not lakehouse.empty:\n","            lakehouse_schema= StructType()\n","            for column in lakehouse:\n","                lakehouse_schema.add(str(column),StringType())\n","        \n","    if not warehouses.empty:\n","            warehouses_schema= StructType()\n","            for column in warehouses:\n","                warehouses_schema.add(str(column),StringType())\n","    \n","    if not synapsenotebook.empty:\n","        synapsenotebook_schema= StructType()\n","        for column in synapsenotebook:\n","            synapsenotebook_schema.add(str(column),StringType())\n","\n","    ############################################ Create Spark dataframe and Tables #################################################\n","    \n","    if not reports.empty:\n","        reportsdf = spark.createDataFrame(reports,schema=reports_schema)\n","        createDeltaTables('Reports',reportsdf)\n","\n","    if not reportUsers.empty:\n","        reportUsersdf = spark.createDataFrame(reportUsers,schema=reportUsers_schema)\n","        createDeltaTables('ReportUsers',reportUsersdf)\n","\n","    if not dashboards.empty:\n","        dashboardsdf=spark.createDataFrame(dashboards,dashboards_schema)\n","        createDeltaTables('Dashboards',dashboardsdf)\n","    \n","    if not dashboardUsers.empty:\n","        dashboardUsersdf=spark.createDataFrame(dashboardUsers,dashboardUsers_schema)\n","        createDeltaTables('DashboardUsers',dashboardUsersdf)\n","    \n","    if not datasets.empty:\n","        datasetsdf=spark.createDataFrame(datasets,datasets_schema)\n","        createDeltaTables('Datasets',datasetsdf)\n","    \n","    if not datasetUsers.empty:\n","        datasetUsersdf=spark.createDataFrame(datasetUsers,datasetUsers_schema)\n","        createDeltaTables('DatasetUsers',datasetUsersdf)\n","\n","    if not dataflows.empty:\n","        dataflowsdf=spark.createDataFrame(dataflows,dataflow_schema)\n","        createDeltaTables('Dataflows',dataflowsdf)\n","\n","    if not dataflowUsers.empty:\n","        dataflowUsersdf=spark.createDataFrame(dataflowUsers,dataflowUsers_schema)\n","        createDeltaTables('DataflowUsers',dataflowUsersdf)\n","\n","    if not lakehouse.empty:\n","        #if \"id\" in lakehouse.columns:\n","         #   lakehouse = lakehouse.filter(lakehouse.id!=\"NaN\")\n","        lakehousedf=spark.createDataFrame(lakehouse,lakehouse_schema)\n","        createDeltaTables('Lakehouses',lakehousedf)\n","\n","    if not warehouses.empty:\n","        #if \"id\" in warehouses.columns:\n","         #   warehouses = warehouses.filter(warehouses.id!=\"NaN\")\n","        warehousesdf=spark.createDataFrame(warehouses,warehouses_schema)\n","        createDeltaTables('Warehouses',warehousesdf)\n","    \n","    if not synapsenotebook.empty:\n","        #if \"id\" in synapsenotebook.columns:\n","         #   synapsenotebook = synapsenotebook.filter(synapsenotebook.id!=\"NaN\")\n","        synapsenotebookdf=spark.createDataFrame(synapsenotebook,synapsenotebook_schema)\n","        createDeltaTables('Synapsenotebooks',synapsenotebookdf)\n","\n","    #datamartsdf = spark.createDataFrame(datamarts)\n","    if not workspaceUsers.empty:\n","        workspaceUsersdf =spark.createDataFrame(workspaceUsers,workspaceUsers_schema)\n","        createDeltaTables('WorkspaceUsers',workspaceUsersdf)\n","    \n","    if not dfdatasourceInstances.empty:\n","        datasourceInstaces =spark.createDataFrame(dfdatasourceInstances,datasourceInstances_schema)\n","        createDeltaTables('datasourceInstances',datasourceInstaces)\n","    \n","    workspacedf=spark.createDataFrame(workspaces,workspace_schema)\n","    createDeltaTables('Workspaces',workspacedf)\n","\n","     #createDeltaTables('datamarts',datamartsdf)   \n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e83c6603-9e0c-414a-9b2e-198917d5ff6d"},{"cell_type":"code","source":["#if not reports.empty:\n","Key=\"t.reports_id = s.reports_id\"\n","createDeltaTablesV1('Reports',Key)\n","\n","try:\n","    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n","    createDeltaTablesV1('ReportUsers',Key)\n","except:\n","    pass\n","#if not reportUsers.empty:\n","\n","\n","#if not dashboards.empty:\n","key=\"t.dashboards_id = s.dashboards_id\"\n","createDeltaTablesV1('Dashboards',key)\n","\n","try:\n","    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n","    createDeltaTablesV1('DashboardUsers',key) \n","except:\n","    vDate=date.today().strftime(\"%Y/%m/%d\")\n","    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n","    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n","        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n","    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n","    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n","        \n","    sdfSource=sdfSource.drop(\"RunDate\")\n","    sdfSource=sdfSource.distinct()\n","    sdfSource=sdfSource.withColumn('RunDate', F.lit(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"))) \n","\n","    (sdfToUpdate.alias(\"t\")\n","        .merge(sdfSource.alias(\"s\"), \"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\")\n","        .whenMatchedUpdateAll()\n","        .whenNotMatchedInsertAll()\n","        .execute()\n","    )\n","\n","#if not dashboardUsers.empty:\n","\n","#if not datasets.empty:\n","key=\"t.datasets_id = s.datasets_id\"\n","createDeltaTablesV1('Datasets',key)\n","\n","#if not datasetUsers.empty:\n","try:\n","    Key=\"t.datasets_id = s.datasets_id AND t.graphId=s.graphId AND t.datasetUserAccessRight=s.datasetUserAccessRight\"\n","    createDeltaTablesV1('DatasetUsers',key) \n","except:\n","    vDate=date.today().strftime(\"%Y/%m/%d\")\n","    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DatasetUsers\" + \"/\"\n","    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DatasetUsers\"\n","        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n","    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n","    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n","        \n","    sdfSource=sdfSource.drop(\"RunDate\")\n","    sdfSource=sdfSource.distinct()\n","    sdfSource=sdfSource.withColumn('RunDate', F.lit(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"))) \n","\n","    (sdfToUpdate.alias(\"t\")\n","        .merge(sdfSource.alias(\"s\"), \"t.datasets_id = s.datasets_id AND t.graphId=s.graphId AND t.datasetUserAccessRight=s.datasetUserAccessRight\")\n","        .whenMatchedUpdateAll()\n","        .whenNotMatchedInsertAll()\n","        .execute()\n","    )\n","\n","#if not dataflows.empty:\n","key=\"t.dataflows_objectId = s.dataflows_objectId\"\n","createDeltaTablesV1('Dataflows',key)\n","\n","#if not dataflowUsers.empty:\n","\n","try:\n","    Key=\"t.dataflows_objectId = s.dataflows_objectId AND t.graphId=s.graphId\"\n","    createDeltaTablesV1('DataflowUsers',key)\n","except:\n","    vDate=date.today().strftime(\"%Y/%m/%d\")\n","    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DataflowUsers\" + \"/\"\n","    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DataflowUsers\"\n","        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n","    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n","    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n","        \n","    sdfSource=sdfSource.drop(\"RunDate\")\n","    sdfSource=sdfSource.distinct()\n","    sdfSource=sdfSource.withColumn('RunDate', F.lit(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"))) \n","\n","    (sdfToUpdate.alias(\"t\")\n","        .merge(sdfSource.alias(\"s\"), \"t.dataflows_objectId = s.dataflows_objectId AND t.graphId=s.graphId\")\n","        .whenMatchedUpdateAll()\n","        .whenNotMatchedInsertAll()\n","        .execute()\n","    )\n","\n","#if not lakehouse.empty:\n","try:\n","    key=\"t.id = s.id\"\n","    createDeltaTablesV1('Lakehouses',key)\n","except:\n","    pass\n","\n","#if not warehouses.empty:\n","try:\n","    key=\"t.id = s.id\"\n","    createDeltaTablesV1('Warehouses',key)\n","except:\n","    pass\n","\n","#if not synapsenotebook.empty:\n","try:\n","    key=\"t.id = s.id\"\n","    createDeltaTablesV1('Synapsenotebooks',key)\n","except:\n","    pass\n","\n","#if not workspaceUsers.empty:\n","\n","try:\n","    key=\"t.id = s.id AND t.users_graphId=s.users_graphId\"\n","    createDeltaTablesV1('WorkspaceUsers',key)\n","except:\n","    pass\n","\n","\n","#if not dfdatasourceInstances.empty:\n","try:\n","    key=\"t.datasourceId = s.datasourceId AND t.gatewayId=s.gatewayId AND s.datasourceType=t.datasourceType\"\n","    createDeltaTablesV1('datasourceInstances',key)\n","except:\n","    pass\n","\n","#if not workspaces.empty:\n","try:\n","    key=\"t.id = s.id\"\n","    createDeltaTablesV1('Workspaces',key)\n","except:\n","    pass"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e8b5a59-5963-4148-a31b-333453510abf","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T14:53:12.3441135Z","session_start_time":null,"execution_start_time":"2023-08-07T14:53:12.7389161Z","execution_finish_time":"2023-08-07T14:59:04.5684913Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":236,"FAILED":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":437,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Compute snapshot for version: 4","submissionTime":"2023-08-07T14:59:04.119GMT","completionTime":"2023-08-07T14:59:04.145GMT","stageIds":[648,649,650],"jobGroup":"11","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":436,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Compute snapshot for version: 4","submissionTime":"2023-08-07T14:59:03.746GMT","completionTime":"2023-08-07T14:59:04.104GMT","stageIds":[647,646],"jobGroup":"11","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":435,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Compute snapshot for version: 4","submissionTime":"2023-08-07T14:59:03.549GMT","completionTime":"2023-08-07T14:59:03.644GMT","stageIds":[645],"jobGroup":"11","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":434,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:59:02.405GMT","completionTime":"2023-08-07T14:59:02.806GMT","stageIds":[643,644,641,642],"jobGroup":"11","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":433,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:59:01.981GMT","completionTime":"2023-08-07T14:59:02.358GMT","stageIds":[640,638,639],"jobGroup":"11","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":432,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:59:01.405GMT","completionTime":"2023-08-07T14:59:01.888GMT","stageIds":[637],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":431,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged no shuffle data","submissionTime":"2023-08-07T14:59:01.329GMT","completionTime":"2023-08-07T14:59:03.062GMT","stageIds":[636],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":430,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:59:01.266GMT","completionTime":"2023-08-07T14:59:01.405GMT","stageIds":[635],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":429,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Finding touched files - low shuffle merge","submissionTime":"2023-08-07T14:58:59.636GMT","completionTime":"2023-08-07T14:59:01.134GMT","stageIds":[633,634],"jobGroup":"11","status":"SUCCEEDED","numTasks":204,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":428,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Finding touched files - low shuffle merge","submissionTime":"2023-08-07T14:58:59.291GMT","completionTime":"2023-08-07T14:58:59.593GMT","stageIds":[632],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":426,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Finding touched files - low shuffle merge","submissionTime":"2023-08-07T14:58:59.037GMT","completionTime":"2023-08-07T14:58:59.125GMT","stageIds":[628,629],"jobGroup":"11","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":425,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...","submissionTime":"2023-08-07T14:58:57.272GMT","completionTime":"2023-08-07T14:58:58.942GMT","stageIds":[627],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at <unknown>:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":424,"name":"load at <unknown>:0","description":"Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...","submissionTime":"2023-08-07T14:58:57.057GMT","completionTime":"2023-08-07T14:58:57.128GMT","stageIds":[626],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":423,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Compute snapshot for version: 5","submissionTime":"2023-08-07T14:58:56.758GMT","completionTime":"2023-08-07T14:58:56.781GMT","stageIds":[625,623,624],"jobGroup":"11","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":56,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":422,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Compute snapshot for version: 5","submissionTime":"2023-08-07T14:58:56.425GMT","completionTime":"2023-08-07T14:58:56.742GMT","stageIds":[622,621],"jobGroup":"11","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":421,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Compute snapshot for version: 5","submissionTime":"2023-08-07T14:58:56.251GMT","completionTime":"2023-08-07T14:58:56.318GMT","stageIds":[620],"jobGroup":"11","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":420,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:58:55.473GMT","completionTime":"2023-08-07T14:58:55.774GMT","stageIds":[619,616,617,618],"jobGroup":"11","status":"SUCCEEDED","numTasks":14,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":13,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":419,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:58:54.977GMT","completionTime":"2023-08-07T14:58:55.421GMT","stageIds":[614,615,613],"jobGroup":"11","status":"SUCCEEDED","numTasks":13,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":418,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged no shuffle data","submissionTime":"2023-08-07T14:58:54.481GMT","completionTime":"2023-08-07T14:58:55.262GMT","stageIds":[612],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":417,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 11:\n#if not reports.empty:\nKey=\"t.reports_id = s.reports_id\"\ncreateDeltaTablesV1('Reports',Key)\n\ntry:\n    Key=\"t.reports_id = s.reports_id AND t.graphId=s.graphId AND t.reportUserAccessRight=s.reportUserAccessRight\"\n    createDeltaTablesV1('ReportUsers',Key)\nexcept:\n    pass\n#if not reportUsers.empty:\n\n\n#if not dashboards.empty:\nkey=\"t.dashboards_id = s.dashboards_id\"\ncreateDeltaTablesV1('Dashboards',key)\n\ntry:\n    Key=\"t.dashboards_id = s.dashboards_id AND t.graphId=s.graphId AND s.dashboardUserAccessRight=t.dashboardUserAccessRight\"\n    createDeltaTablesV1('DashboardUsers',key) \nexcept:\n    vDate=date.today().strftime(\"%Y/%m/%d\")\n    toUpdatePath=\"/2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda/Tables/\" + \"DashboardUsers\" + \"/\"\n    sourcePath=\"Files/Stage/\" + vDate + \"/\" + \"DashboardUsers\"\n        #sourcePath=\"Files/Stage/2023/07/22/\" +  tableName\n    sdfToUpdate = DeltaTable.forPath(spark,toUpdatePath)\n    sdfSource= spark.read.format(\"parquet\").load(sourcePath)\n        \n    sdfSource=sdfSource.drop(\"RunDate\")\n    sdfSo...: Writing merged data full update","submissionTime":"2023-08-07T14:58:54.455GMT","completionTime":"2023-08-07T14:58:54.823GMT","stageIds":[611],"jobGroup":"11","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2da695d4-4db8-48c2-86b2-08c83e6d3254"},"text/plain":"StatementMeta(, 1e8b5a59-5963-4148-a31b-333453510abf, 11, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"1"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"8b128f74-b83b-4f93-bf07-d25b123a0c5c\",\"activityId\":\"1e8b5a59-5963-4148-a31b-333453510abf\",\"applicationId\":\"application_1691415978124_0001\",\"jobGroupId\":\"11\",\"advices\":{\"warn\":7}}"}},"id":"db381df5-fcc8-44c8-aaf8-a02c488fd827"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda","known_lakehouses":[{"id":"2fe98f6a-0a9a-4a7c-bb43-37c1cbc58fda"}],"default_lakehouse_name":"StagePBIAssessment","default_lakehouse_workspace_id":"4907d72a-0330-4d27-aa1a-f6e9484ba3f1"}}},"nbformat":4,"nbformat_minor":5}